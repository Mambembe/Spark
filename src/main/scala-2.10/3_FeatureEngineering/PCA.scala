package PreparingData

import org.apache.spark.SparkContext
import org.apache.spark.sql.SQLContext
import org.apache.spark.ml.feature.VectorAssembler
import org.apache.spark.mllib.linalg.{Vector, Vectors, Matrix}
import org.apache.spark.mllib.linalg.distributed.RowMatrix


object PCA {

  def main(args: Array[String]): Unit = {


    val sc = new SparkContext("local[*]", "PCA")

    val sqlContext = new SQLContext(sc)
    //import sqlContext.implicits._
    sc.setLogLevel("WARN")

    val crimes = sqlContext.read
      .format("com.databricks.spark.csv")
      .option("delimiter", ",")
      .option("header", "true") // Use first line of all files as header
      .option("inferSchema", "true") // Automatically infer data types
      .load("data/UScrime2-colsLotsOfNAremoved.csv")
    // Convert "OtherPerCap"-column to double
    // Drops the first column of strings
    val df_1 = crimes.withColumnRenamed("OtherPerCap","OtherPerCapold")
    val df_2 = df_1.withColumn("OtherPerCap",df_1.col("OtherPerCapold").cast("double")).drop("OtherPerCapold")
    val finalCrimes = df_2.drop("community").na.drop()

    finalCrimes.printSchema()
    finalCrimes.show()
    println("Number of rows: " + finalCrimes.count())
    println("Number of columns: " + finalCrimes.columns.length)

    /**
      * VectorAssembler is a transformer that combines a given list of columns into a
      * single vector column. It is useful for combining raw features and features
      * generated by different feature transformers into a single feature vector, in
      * order to train ML models
      * PCA wants a rowmatrix and not a dataframe
      */
    val assembler = new VectorAssembler()
      .setInputCols(finalCrimes.columns)
      .setOutputCol("features")

    val featuresDF = assembler.transform(finalCrimes)
    println(featuresDF.select("features").first())

    val rddOfRows = featuresDF.rdd

    // Take the rdd that represent the feature dataframe and applies a function to each
    // value to convert it to a vector
    val rddOfVectors = rddOfRows.map(row => row.get(100).asInstanceOf[Vector])

    //rddOfVectors.foreach(x=>println(x))
    val mat = new RowMatrix(rddOfVectors)

    // Principal components are stored in a local dense matrix
    val pc: Matrix = mat.computePrincipalComponents(10)

    print(pc)
    sc.stop()
  }
}
